# app.py â€” ë³´í—˜ ì•½ê´€ RAG ì±—ë´‡ (Cosine Similarity, Supabase pgvector, GPT-5)
import os
import json
import time
import typing as t
import numpy as np
import psycopg
import pandas as pd
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.documents import Document

# =========================
# ğŸ”§ í™˜ê²½ ë³€ìˆ˜
# =========================
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
DB_HOST = os.getenv("DB_HOST") or st.secrets.get("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT") or st.secrets.get("DB_PORT", 5432))
DB_NAME = os.getenv("DB_NAME") or st.secrets.get("DB_NAME")
DB_USER = os.getenv("DB_USER") or st.secrets.get("DB_USER")
DB_PASS = os.getenv("DB_PASS") or st.secrets.get("DB_PASS")

missing = [k for k, v in {
    "OPENAI_API_KEY": OPENAI_API_KEY,
    "DB_HOST": DB_HOST, "DB_PORT": DB_PORT,
    "DB_NAME": DB_NAME, "DB_USER": DB_USER, "DB_PASS": DB_PASS
}.items() if not v]
if missing:
    st.error(f"í™˜ê²½ ë³€ìˆ˜ ëˆ„ë½: {', '.join(missing)}")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)
model = ChatOpenAI(model='gpt-5', reasoning_effort='minimal', api_key=OPENAI_API_KEY)

# =========================
# ğŸ§± Streamlit UI
# =========================
st.set_page_config(page_title="ì•½ê´€ RAG ì±—ë´‡", page_icon="ğŸ¤–", layout="wide")
st.markdown("""
<style>
  .glow-input input { box-shadow: 0 0 12px rgba(0,120,255,0.45); border-radius: 999px; }
  .small { font-size: 0.9rem; color: #666; }
  .cite { font-size: 0.9rem; color: #3b82f6; }
  .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono"; }
</style>
""", unsafe_allow_html=True)

col1, col2 = st.columns([0.9, 0.1])
with col1:
    st.markdown("## ğŸ¤– ë³´í—˜ ì•½ê´€ RAG ì±—ë´‡")
    st.caption("Supabase(pgvector)ì—ì„œ ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰ â†’ GPT-5 ë‹µë³€ (Cosine Similarity)")
with col2:
    with st.popover("âš™ï¸ ì˜µì…˜"):
        top_k = st.slider("ê²€ìƒ‰ Top-K", 1, 20, 10, 1)
        max_ctx_chars = st.slider("ì»¨í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´(ë¬¸ì)", 2000, 12000, 6000, 500)
        temperature = st.slider("ì°½ì˜ì„±(Temperature)", 0.0, 1.2, 0.2, 0.1)
        sys_style = st.selectbox("ì‘ë‹µ ìŠ¤íƒ€ì¼",
                                 ["ê°„ê²° ìš”ì•½", "ê·¼ê±° ì¤‘ì‹¬", "ì¹œì ˆ ì„¤ëª…"],
                                 index=1)

st.markdown('<div class="glow-input">', unsafe_allow_html=True)
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 369ë‰´í…Œí¬NHì•”ë³´í—˜ ì•”ìˆ˜ìˆ ìê¸ˆì€ ì–¼ë§ˆì¸ê°€ìš”?)", "")
st.markdown("</div>", unsafe_allow_html=True)
ask = st.button("ì‹¤í–‰", type="primary", use_container_width=True)

# =========================
# ğŸ—„ï¸ DB ì—°ê²° ë° Cosine ê²€ìƒ‰ SQL
# =========================
DB_CONN = {
    "host": DB_HOST,
    "port": DB_PORT,
    "dbname": DB_NAME,
    "user": DB_USER,
    "password": DB_PASS,
    "sslmode": "require",
    "connect_timeout": 10,
    "options": "-c prepare_threshold=0 -c tcp_keepalives_idle=20 "
               "-c tcp_keepalives_interval=10 -c tcp_keepalives_count=3",
}

SEARCH_SQL = """
WITH q AS (SELECT %(vec)s::vector AS v)
SELECT
  pdf_filename,
  COALESCE((metadata_json->>'pdf_path'), pdf_path) AS pdf_path,
  page,
  chunk_id,
  text,
  metadata_json,
  (1 - (embedding <=> (SELECT v FROM q))) AS cosine_similarity
FROM public.terms_chunks
ORDER BY cosine_similarity DESC
LIMIT %(k)s;
"""

def to_vector_literal(vec: t.List[float]) -> str:
    return "[" + ",".join(f"{x:.8f}" for x in vec) + "]"

def embed_text(text: str, model_name: str = "text-embedding-3-small") -> t.List[float]:
    e = client.embeddings.create(model=model_name, input=text)
    v = np.array(e.data[0].embedding, dtype=np.float32)
    norm = np.linalg.norm(v)
    return (v / norm).tolist() if norm > 0 else v.tolist()

def fetch_topk_chunks(q_vec: t.List[float], k: int) -> pd.DataFrame:
    vec_literal = to_vector_literal(q_vec)
    with psycopg.connect(**DB_CONN) as conn, conn.cursor() as cur:
        cur.execute(SEARCH_SQL, {"vec": vec_literal, "k": k})
        rows = cur.fetchall()
        cols = [d.name for d in cur.description]
    return pd.DataFrame(rows, columns=cols)

# =========================
# ğŸ“¦ VectorStore ì–´ëŒ‘í„° (DB ê¸°ë°˜)
# =========================
class DBStore:
    """LangChain VectorStoreì²˜ëŸ¼ ë™ì‘í•˜ëŠ” ì–´ëŒ‘í„°."""
    def __init__(self, default_k: int = 10):
        self.default_k = default_k
        self.last_df: pd.DataFrame | None = None

    def similarity_search(self, query: str, k: int | None = None) -> list[Document]:
        k = k or self.default_k
        q_vec = embed_text(query)
        df = fetch_topk_chunks(q_vec, k)
        self.last_df = df
        docs: list[Document] = []
        for _, r in df.iterrows():
            meta = {
                "pdf_filename": r["pdf_filename"],
                "pdf_path": r.get("pdf_path"),
                "page": int(r.get("page") or 0),
                "chunk_id": r.get("chunk_id"),
                "cosine_similarity": float(r.get("cosine_similarity") or 0.0),
                "metadata_json": r.get("metadata_json"),
            }
            docs.append(Document(page_content=str(r["text"] or "").strip(), metadata=meta))
        return docs

store = DBStore(default_k=10)

# =========================
# ğŸ’¬ ì§ˆì˜ í•¨ìˆ˜ (Top 3 ì»¨í…ìŠ¤íŠ¸ ë²„ì „)
# =========================
def query(question: str) -> str:
    store.default_k = top_k  # UI ê°’ ë°˜ì˜
    results = store.similarity_search(question)
    if not results:
        return "ìœ ì‚¬í•œ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”."

    # âœ… ìƒìœ„ 3ê°œì˜ ì²­í¬ë¥¼ ëª¨ë‘ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
    top_n = min(3, len(results))
    combined_context = "\n\n---\n\n".join(
        f"[{i+1}] {r.page_content}" for i, r in enumerate(results[:top_n])
    )

    system = f"""
    ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë³´í—˜ì‚¬ì˜ ê³µì‹ ì•½ê´€ì„ ì´í•´í•˜ê³  ì„¤ëª…í•˜ëŠ” ì „ë¬¸ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
    ì•„ë˜ëŠ” ë³´í—˜ì•½ê´€ì—ì„œ ì¶”ì¶œëœ Top {top_n}ê°œì˜ ê´€ë ¨ ì¡°í•­ì…ë‹ˆë‹¤.
    ê° ì¡°í•­ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ **ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆëŠ” ë‹µë³€**ì„ ì‘ì„±í•˜ì„¸ìš”.

    [ì§€ì¹¨]
    1. ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ ì•½ê´€ ë‚´ìš©(ì»¨í…ìŠ¤íŠ¸)ì— ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤.
    2. ê·¼ê±°ê°€ ëª…í™•íˆ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ â€œì•½ê´€ì—ì„œ í•´ë‹¹ ë‚´ìš©ì€ ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.â€ë¼ê³  ë‹µí•˜ì„¸ìš”.
    3. ëª¨í˜¸í•˜ê±°ë‚˜ ì¤‘ë³µëœ í‘œí˜„ì€ ì •ë¦¬í•˜ê³ , í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
    4. ìˆ«ì(ë³´í—˜ê¸ˆ, ì§€ê¸‰í•œë„, ê¸°ê°„ ë“±)ëŠ” ì•½ê´€ ë‚´ í‘œê¸°ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.
    5. ì‚¬ìš©ìê°€ ì‹¤ì œ ë³´í—˜ê³„ì•½ìë¼ê³  ê°€ì •í•˜ê³ , ì´í•´í•˜ê¸° ì‰½ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
    6. ë²•ë¥ ì , ê³„ì•½ì  í‘œí˜„ì€ ì¡´ì¹­ì²´ë¡œ ë‹µë³€í•˜ì„¸ìš”. (ì˜ˆ: â€œì§€ê¸‰ë©ë‹ˆë‹¤.â€, â€œí•´ë‹¹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.â€)
    7. ëì— ê°„ë‹¨íˆ ìš”ì•½ ë¬¸ì¥ì„ ë§ë¶™ì´ì„¸ìš”. (ì˜ˆ: â€œìš”ì•½í•˜ë©´, ì•”ìˆ˜ìˆ ìê¸ˆì€ ì§„ë‹¨ í›„ 1íšŒ ì§€ê¸‰ë©ë‹ˆë‹¤.â€)

    [ì•½ê´€ ê´€ë ¨ ì¡°í•­ ìš”ì•½]
    {combined_context}
    """.strip()

    messages = [
        SystemMessage(system),
        HumanMessage(question)
    ]

    resp = model.invoke(messages)
    return resp.content

# =========================
# ğŸ“‘ ê·¼ê±° í‘œì‹œ
# =========================
def render_citations(df: pd.DataFrame):
    if df.empty:
        return
    st.markdown("**ì°¸ê³ í•œ ê·¼ê±° (Top-K):**")
    for i, r in df.iterrows():
        st.markdown(
            f"- <span class='cite'>[{i+1}] {r['pdf_filename']} p.{int(r['page'])}</span> "
            f"<span class='small'>(ìœ ì‚¬ë„: {r['cosine_similarity']:.4f})</span>",
            unsafe_allow_html=True
        )

# =========================
# ğŸš€ ì‹¤í–‰ë¶€
# =========================
if ask:
    if not question.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    with st.spinner("ê²€ìƒ‰ ë° GPT-5 ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
            answer = query(question)
        except Exception as e:
            st.error(f"ì˜¤ë¥˜: {e}")
            st.stop()

    st.markdown("### ğŸ“Œ ë‹µë³€")
    st.write(answer)
    st.divider()

    if getattr(store, "last_df", None) is not None and not store.last_df.empty:
        render_citations(store.last_df)
    else:
        st.caption("ê·¼ê±°ë¥¼ í‘œì‹œí•  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    st.info("ì¢Œì¸¡ ì…ë ¥ì°½ì— ì§ˆë¬¸ì„ ì“°ê³  **ì‹¤í–‰**ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
