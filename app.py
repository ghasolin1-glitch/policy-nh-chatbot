import os, json, time, numpy as np, psycopg, pandas as pd, streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

# =========================
# ğŸ”§ í™˜ê²½ ë³€ìˆ˜
# =========================
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
DB_HOST = os.getenv("DB_HOST") or st.secrets.get("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT") or st.secrets.get("DB_PORT", 5432))
DB_NAME = os.getenv("DB_NAME") or st.secrets.get("DB_NAME")
DB_USER = os.getenv("DB_USER") or st.secrets.get("DB_USER")
DB_PASS = os.getenv("DB_PASS") or st.secrets.get("DB_PASS")

client = OpenAI(api_key=OPENAI_API_KEY)
model = ChatOpenAI(model="gpt-5", reasoning_effort="minimal", api_key=OPENAI_API_KEY)

# =========================
# ğŸ“¦ ì„¸ì…˜ ìƒíƒœ
# =========================
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}]

# =========================
# ğŸ“¦ DB ì—°ê²° í•¨ìˆ˜
# =========================
DB_CONN = {
    "host": DB_HOST,
    "port": DB_PORT,
    "dbname": DB_NAME,
    "user": DB_USER,
    "password": DB_PASS,
    "sslmode": "require",
    "connect_timeout": 10,
}

SEARCH_SQL = """
WITH q AS (SELECT %(vec)s::vector AS v)
SELECT pdf_filename, COALESCE((metadata_json->>'pdf_path'), pdf_path) AS pdf_path, 
       page, text, (1 - (embedding <=> (SELECT v FROM q))) AS cosine_similarity
FROM public.terms_chunks
ORDER BY cosine_similarity DESC
LIMIT %(k)s;
"""

def to_vector_literal(vec):
    return "[" + ",".join(f"{x:.8f}" for x in vec) + "]"

def embed_text(text: str) -> list[float]:
    e = client.embeddings.create(model="text-embedding-3-small", input=text)
    v = np.array(e.data[0].embedding, dtype=np.float32)
    norm = np.linalg.norm(v)
    return (v / norm).tolist() if norm > 0 else v.tolist()

def fetch_topk_chunks(q_vec, k=5):
    vec_literal = to_vector_literal(q_vec)
    with psycopg.connect(**DB_CONN) as conn, conn.cursor() as cur:
        cur.execute(SEARCH_SQL, {"vec": vec_literal, "k": k})
        rows = cur.fetchall()
        cols = [d.name for d in cur.description]
    return pd.DataFrame(rows, columns=cols)

# =========================
# ğŸ’¬ ë‹µë³€ í•¨ìˆ˜
# =========================
# =========================
# ğŸ’¬ ì§ˆì˜ í•¨ìˆ˜ (RAG ê¸°ë°˜)
# =========================
def query(question: str) -> str:
    try:
        # âœ… ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ì„ë² ë”©
        q_vec = embed_text(question)
        df = fetch_topk_chunks(q_vec, 5)

        if df.empty:
            return "ìœ ì‚¬í•œ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì…ë ¥í•´ ì£¼ì„¸ìš”."

        # âœ… ìƒìœ„ 3ê°œì˜ ì²­í¬ë¥¼ ëª¨ë‘ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©
        top_n = min(3, len(df))
        combined_context = "\n\n---\n\n".join(
            f"[{i+1}] {row['text']}" for i, row in df.head(top_n).iterrows()
        )

        system = f"""
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë³´í—˜ì‚¬ì˜ ê³µì‹ ì•½ê´€ì„ ì´í•´í•˜ê³  ì„¤ëª…í•˜ëŠ” ì „ë¬¸ AI ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì•„ë˜ëŠ” ë³´í—˜ì•½ê´€ì—ì„œ ì¶”ì¶œëœ Top {top_n}ê°œì˜ ê´€ë ¨ ì¡°í•­ì…ë‹ˆë‹¤.
ê° ì¡°í•­ì„ ì¢…í•©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ **ì •í™•í•˜ê³  ì‹ ë¢°ì„± ìˆëŠ” ë‹µë³€**ì„ ì‘ì„±í•˜ì„¸ìš”.

[ì§€ì¹¨]
1. ë‹µë³€ì€ ë°˜ë“œì‹œ ì•„ë˜ ì•½ê´€ ë‚´ìš©(ì»¨í…ìŠ¤íŠ¸)ì— ê·¼ê±°í•´ì•¼ í•©ë‹ˆë‹¤.
2. ê·¼ê±°ê°€ ëª…í™•íˆ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ â€œì•½ê´€ì—ì„œ í•´ë‹¹ ë‚´ìš©ì€ ëª…ì‹œë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.â€ë¼ê³  ë‹µí•˜ì„¸ìš”.
3. ëª¨í˜¸í•˜ê±°ë‚˜ ì¤‘ë³µëœ í‘œí˜„ì€ ì •ë¦¬í•˜ê³ , í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
4. ìˆ«ì(ë³´í—˜ê¸ˆ, ì§€ê¸‰í•œë„, ê¸°ê°„ ë“±)ëŠ” ì•½ê´€ ë‚´ í‘œê¸°ëŒ€ë¡œ ìœ ì§€í•˜ì„¸ìš”.
5. ì‚¬ìš©ìê°€ ì‹¤ì œ ë³´í—˜ê³„ì•½ìë¼ê³  ê°€ì •í•˜ê³ , ì´í•´í•˜ê¸° ì‰½ê²Œ ìì—°ìŠ¤ëŸ½ê²Œ ì„¤ëª…í•˜ì„¸ìš”.
6. ë²•ë¥ ì , ê³„ì•½ì  í‘œí˜„ì€ ì¡´ì¹­ì²´ë¡œ ë‹µë³€í•˜ì„¸ìš”. (ì˜ˆ: â€œì§€ê¸‰ë©ë‹ˆë‹¤.â€, â€œí•´ë‹¹ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.â€)
7. ëì— ê°„ë‹¨íˆ ìš”ì•½ ë¬¸ì¥ì„ ë§ë¶™ì´ì„¸ìš”. (ì˜ˆ: â€œìš”ì•½í•˜ë©´, ì•”ìˆ˜ìˆ ìê¸ˆì€ ì§„ë‹¨ í›„ 1íšŒ ì§€ê¸‰ë©ë‹ˆë‹¤.â€)

[ì•½ê´€ ê´€ë ¨ ì¡°í•­ ìš”ì•½]
{combined_context}
""".strip()

        messages = [
            SystemMessage(system),
            HumanMessage(question)
        ]

        resp = model.invoke(messages)
        return resp.content

    except Exception as e:
        return f"âš ï¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

# =========================
# ğŸ¨ UI êµ¬ì„± (Streamlit)
# =========================
st.markdown("""
<style>
body { background-color: #f3f4f6; font-family: Pretendard, Inter, sans-serif; }

/* ì±„íŒ… ë°•ìŠ¤ */
.chat-box {
    background: white; border-radius: 10px;
    box-shadow: 0 4px 12px rgba(0,0,0,0.1);
    width: 100%; max-width: 480px; margin: auto; padding: 16px;
}

/* íƒ€ì´í‹€ ë°•ìŠ¤ - ë†’ì´ 80% */
.chat-header {
    background-color: #2563eb; color: white;
    padding: 8px 14px;          /* ìœ„ì•„ë˜ padding ì¤„ì„ (80%) */
    border-radius: 8px; margin-bottom: 10px;
    text-align: center;
}

/* ë§í’ì„  ìŠ¤íƒ€ì¼ */
.user-bubble {
    background-color: #2563eb; color: white;
    border-radius: 20px 20px 0 20px;
    padding: 10px 14px; margin-bottom: 8px;
    margin-left: auto; max-width: 80%;
    box-shadow: 0 2px 6px rgba(0,0,0,0.15);
}
.bot-bubble {
    background-color: #e5e7eb; color: #111827;
    border-radius: 20px 20px 20px 0;
    padding: 10px 14px; margin-bottom: 8px;
    margin-right: auto; max-width: 80%;
    box-shadow: 0 2px 6px rgba(0,0,0,0.1);
}

/* ì…ë ¥ì°½ */
.chat-input {
    border: 2px solid #2563eb; border-radius: 20px;
    padding: 10px 16px; width: 100%;
    font-size: 15px; outline: none;
    box-shadow: 0 2px 6px rgba(37,99,235,0.25);
}

/* ë³´ë‚´ê¸° ë²„íŠ¼ ì¤‘ì•™ ì •ë ¬ */
.send-btn {
    display: flex;
    justify-content: center;  /* ê°€ìš´ë° ì •ë ¬ */
    margin-top: 10px;
}
</style>
""", unsafe_allow_html=True)


with st.container():
    st.markdown("<div class='chat-box'>", unsafe_allow_html=True)
    st.markdown("<div class='chat-header'><h3>ë³´í—˜ì•½ê´€ ì±—ë´‡</h3><p>NHLife | Made by íƒœí›ˆ,í˜„ì² </p></div>", unsafe_allow_html=True)

    for msg in st.session_state.messages:
        bubble_class = "user-bubble" if msg["role"] == "user" else "bot-bubble"
        st.markdown(f"<div class='{bubble_class}'>{msg['content']}</div>", unsafe_allow_html=True)

    st.markdown("<hr>", unsafe_allow_html=True)
    with st.form("chat_form", clear_on_submit=True):
        user_input = st.text_input("", placeholder="ìƒí’ˆì— ëŒ€í•´ ê¶ê¸ˆí•œ ì  ì§ˆë¬¸í•´ì£¼ì„¸ìš”.", label_visibility="collapsed")
        st.markdown("<div class='send-btn'>", unsafe_allow_html=True)
        submitted = st.form_submit_button("ğŸ“ ë³´ë‚´ê¸°")
        st.markdown("</div>", unsafe_allow_html=True)

    st.markdown("</div>", unsafe_allow_html=True)


# =========================
# ğŸ’¬ ì…ë ¥ ì²˜ë¦¬ (Streamlit ì´ë²¤íŠ¸)
# =========================
if submitted and user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})
    # ë‹µë³€ ìƒì„± ì „ íŒŒë€ìƒ‰ ë°•ìŠ¤ í‘œì‹œ
    with st.spinner("ğŸ’™ ì•½ê´€ ë‚´ìš©ì„ ê²€í†  ì¤‘ì…ë‹ˆë‹¤..."):
        st.session_state.messages.append({"role": "assistant", "content": "ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš” ğŸ’¬"})
        st.rerun()

if len(st.session_state.messages) >= 2 and st.session_state.messages[-1]["content"].startswith("ë‹µë³€ì„ ìƒì„± ì¤‘"):
    # ì‹¤ì œ ë‹µë³€ ìƒì„± (generate_answer â†’ query ë¡œ êµì²´)
    question = st.session_state.messages[-2]["content"]
    answer = query(question)
    st.session_state.messages[-1] = {"role": "assistant", "content": answer}
    st.rerun()