# app.py â€” Streamlit RAG Chatbot (Supabase pgvector ê²€ìƒ‰, ì €ì¥ ë¡œì§ ì—†ìŒ)
import os
import json
import time
import typing as t
import psycopg
import pandas as pd
import streamlit as st
from openai import OpenAI
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage


load_dotenv()





# =========================
# ğŸ”§ í™˜ê²½ë³€ìˆ˜ / secrets ì½ê¸°
# =========================
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
DB_HOST = os.getenv("DB_HOST") or st.secrets.get("DB_HOST")
DB_PORT = int(os.getenv("DB_PORT") or st.secrets.get("DB_PORT", 5432))
DB_NAME = os.getenv("DB_NAME") or st.secrets.get("DB_NAME")
DB_USER = os.getenv("DB_USER") or st.secrets.get("DB_USER")
DB_PASS = os.getenv("DB_PASS") or st.secrets.get("DB_PASS")



# í•„ìˆ˜ ê°’ ì²´í¬
missing = [k for k, v in {
    "OPENAI_API_KEY": OPENAI_API_KEY,
    "DB_HOST": DB_HOST, "DB_PORT": DB_PORT, "DB_NAME": DB_NAME, "DB_USER": DB_USER, "DB_PASS": DB_PASS
}.items() if not v]
if missing:
    st.error(f"í™˜ê²½ë³€ìˆ˜/ì‹œí¬ë¦¿ ëˆ„ë½: {', '.join(missing)}")
    st.stop()

client = OpenAI(api_key=OPENAI_API_KEY)

# ì „ì—­ ëª¨ë¸ ê°ì²´ ìƒì„± (Streamlit ì‹¤í–‰ ì‹œ 1íšŒë§Œ)
model = ChatOpenAI(
    model="gpt-5",
    reasoning_effort="minimal",
    api_key=OPENAI_API_KEY
)


# =========================
# ğŸ§± Streamlit UI
# =========================
st.set_page_config(page_title="ì•½ê´€ RAG ì±—ë´‡", page_icon="ğŸ¤–", layout="wide")
st.markdown(
    """
    <style>
      .glow-input input { box-shadow: 0 0 12px rgba(0,120,255,0.45); border-radius: 999px; }
      .small { font-size: 0.9rem; color: #666; }
      .cite { font-size: 0.9rem; color: #3b82f6; }
      .mono { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", monospace; }
    </style>
    """,
    unsafe_allow_html=True,
)

col1, col2 = st.columns([0.9, 0.1])
with col1:
    st.markdown("## ğŸ¤– ë³´í—˜ ì•½ê´€ RAG ì±—ë´‡")
    st.caption("Supabase(pgvector)ì—ì„œ ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰ â†’ GPT-5 ë‹µë³€ (ì €ì¥/ì¸ì„œíŠ¸ ì—†ìŒ)")

with col2:
    with st.popover("âš™ï¸ ì˜µì…˜"):
        top_k = st.slider("ê²€ìƒ‰ Top-K", 1, 8, 4, 1)
        max_ctx_chars = st.slider("ì»¨í…ìŠ¤íŠ¸ ìµœëŒ€ ê¸¸ì´(ë¬¸ì)", 2000, 12000, 6000, 500)
        temperature = st.slider("ì°½ì˜ì„±(Temperature)", 0.0, 1.2, 0.2, 0.1)
        sys_style = st.selectbox(
            "ì‘ë‹µ ìŠ¤íƒ€ì¼",
            ["ê°„ê²° ìš”ì•½", "ê·¼ê±° ì¤‘ì‹¬", "ì¹œì ˆ ì„¤ëª…"],
            index=1
        )

st.markdown('<div class="glow-input">', unsafe_allow_html=True)
question = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 369ë‰´í…Œí¬NHì•”ë³´í—˜ ì•”ìˆ˜ìˆ ìê¸ˆì€ ì–¼ë§ˆì¸ê°€ìš”?)", "")
st.markdown("</div>", unsafe_allow_html=True)

ask = st.button("ì‹¤í–‰", type="primary", use_container_width=True)

# =========================
# ğŸ—„ï¸ DB ì—°ê²° ì¤€ë¹„
# =========================
DB_CONN: dict = {
    "host": DB_HOST,
    "port": DB_PORT,
    "dbname": DB_NAME,
    "user": DB_USER,
    "password": DB_PASS,
    "sslmode": "require",
    "connect_timeout": 10,
    # pgbouncer(í’€ëŸ¬) ì—°ê²° ì•ˆì •í™” ì˜µì…˜
    "options": "-c prepare_threshold=0 -c tcp_keepalives_idle=20 -c tcp_keepalives_interval=10 -c tcp_keepalives_count=3",
}

SEARCH_SQL = """
WITH q AS (
  SELECT {vec}::vector AS v
)
SELECT
  pdf_filename,
  COALESCE((metadata_json->>'pdf_path'), pdf_path) AS pdf_path,
  page,
  chunk_id,
  text,
  metadata_json,
  (embedding <=> (SELECT v FROM q)) AS cosine_distance
FROM public.terms_chunks
ORDER BY cosine_distance ASC
LIMIT %(k)s;
"""

def to_vector_literal(vec: t.List[float]) -> str:
    # pgvector ì…ë ¥ í¬ë§·: [0.1, 0.2, ...]
    return "[" + ",".join(f"{x:.8f}" for x in vec) + "]"

def embed_text(text: str, model: str = "text-embedding-3-small") -> t.List[float]:
    e = client.embeddings.create(model=model, input=text)
    return e.data[0].embedding

def fetch_topk_chunks(q_vec: t.List[float], k: int) -> pd.DataFrame:
    vec_literal = to_vector_literal(q_vec)
    # SQL ì•ˆì—ì„œ vec_literalì„ ë¬¸ìì—´ í¬ë§·ìœ¼ë¡œ ì§ì ‘ ì‚½ì…
    sql_stmt = SEARCH_SQL.format(vec=f"'{vec_literal}'")

    with psycopg.connect(**DB_CONN) as conn, conn.cursor() as cur:
        cur.execute(sql_stmt, {"k": k})
        rows = cur.fetchall()
        cols = [d.name for d in cur.description]
    return pd.DataFrame(rows, columns=cols)


def build_context(df: pd.DataFrame, max_chars: int = 6000) -> str:
    parts = []
    total = 0
    for _, r in df.iterrows():
        header = f"[{r['pdf_filename']} p.{int(r['page'])}] {r['chunk_id']}"
        chunk = str(r["text"]).strip()
        block = f"### {header}\n{chunk}"
        if total + len(block) > max_chars:
            break
        parts.append(block)
        total += len(block)
    return "\n\n---\n\n".join(parts)

def system_prompt(style: str) -> str:
    base = (
        "ë‹¹ì‹ ì€ ë³´í—˜ ì•½ê´€ ì „ë¬¸ ìš”ì•½/ì„¤ëª… ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê·¼ê±°ë¡œë§Œ ë‹µí•˜ì„¸ìš”.\n"
        "- ê·¼ê±° ì—†ëŠ” ì¶”ì¸¡/ìƒìƒ ê¸ˆì§€, ìˆ«ìëŠ” ì›ë¬¸ ê¸°ì¤€ìœ¼ë¡œ ì •í™•íˆ.\n"
        "- ì¶œì²˜(íŒŒì¼ëª…Â·í˜ì´ì§€)ë¥¼ í•¨ê»˜ í‘œê¸°í•˜ì„¸ìš”.\n"
    )
    if style == "ê°„ê²° ìš”ì•½":
        base += "- í•œê¸€ë¡œ 3~6ë¬¸ì¥, í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì •ë¦¬.\n"
    elif style == "ê·¼ê±° ì¤‘ì‹¬":
        base += "- ë‹µë³€ ë’¤ì— ê´€ë ¨ ì¡°í•­/ë¬¸êµ¬ë¥¼ ì§§ê²Œ ì¸ìš©(ë”°ì˜´í‘œ)í•˜ê³  ì¶œì²˜ë¥¼ ëª…ì‹œ.\n"
    elif style == "ì¹œì ˆ ì„¤ëª…":
        base += "- ë°°ê²½ê³¼ ì¡°ê±´ì„ ì‰½ê³  ì¹œì ˆí•˜ê²Œ í’€ì–´ ì„¤ëª….\n"
    return base

def chat_answer(question: str, context: str) -> str:
    """ì›ë˜ ì½”ë“œì™€ ë™ì¼í•œ êµ¬ì¡°ì˜ GPT í˜¸ì¶œ"""
    system_prompt = f"ì•„ë˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€ë‹µí•´ì¤˜.\n\n{context}"

    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=question)
    ]

    resp = model.invoke(messages)
    return resp.content


def render_citations(df: pd.DataFrame):
    if df.empty:
        return
    st.markdown("**ì°¸ê³ í•œ ê·¼ê±° (Top-K):**")
    for i, r in df.iterrows():
        sim = 1 - float(r["cosine_distance"])  # ìœ ì‚¬ë„(1 - distance)
        st.markdown(
            f"- <span class='cite'>[{i+1}] {r['pdf_filename']} p.{int(r['page'])}</span> "
            f"<span class='small'>(ìœ ì‚¬ë„: {sim:.4f})</span>",
            unsafe_allow_html=True
        )

# =========================
# ğŸš€ ì‹¤í–‰
# =========================
if ask:
    if not question.strip():
        st.warning("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    with st.spinner("ì§ˆë¬¸ ì„ë² ë”© ìƒì„± ì¤‘..."):
        q_vec = embed_text(question)

    with st.spinner("Supabaseì—ì„œ ìœ ì‚¬ ì²­í¬ ê²€ìƒ‰ ì¤‘..."):
        try:
            df_hits = fetch_topk_chunks(q_vec, top_k)
        except Exception as e:
            st.error(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            st.stop()

    if df_hits.empty:
        st.info("ìœ ì‚¬í•œ ì²­í¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë°”ê¿”ë³´ì„¸ìš”.")
        st.stop()

    ctx = build_context(df_hits, max_chars=max_ctx_chars)

    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        try:
            answer = chat_answer(question, ctx)
        except Exception as e:
            st.error(f"ëª¨ë¸ í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            st.stop()

    st.markdown("### ğŸ“Œ ë‹µë³€")
    st.write(answer)
    st.divider()
    render_citations(df_hits)

    with st.expander("ğŸ” ê²€ìƒ‰ ì»¨í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°"):
        st.markdown(f"<div class='mono' style='white-space:pre-wrap'>{ctx}</div>", unsafe_allow_html=True)

else:
    st.info("ì¢Œì¸¡ ì…ë ¥ì°½ì— ì§ˆë¬¸ì„ ì“°ê³  **ì‹¤í–‰**ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
